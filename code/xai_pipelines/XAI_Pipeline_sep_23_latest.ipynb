{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5260c-0b55-43d5-a262-ed136043a79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:19:58.433246Z",
     "iopub.status.busy": "2023-09-21T18:19:58.432283Z",
     "iopub.status.idle": "2023-09-21T18:20:21.574179Z",
     "shell.execute_reply": "2023-09-21T18:20:21.571995Z"
    },
    "papermill": {
     "duration": 23.201431,
     "end_time": "2023-09-21T18:20:21.580239",
     "exception": false,
     "start_time": "2023-09-21T18:19:58.378808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# !pip install lime\n",
    "# os.chdir(\"../\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# import lime\n",
    "from lime import lime_tabular\n",
    "import shap\n",
    "\n",
    "import json\n",
    "from typing import Union\n",
    "from datetime import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317054a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:21.740832Z",
     "iopub.status.busy": "2023-09-21T18:20:21.739181Z",
     "iopub.status.idle": "2023-09-21T18:20:21.752476Z",
     "shell.execute_reply": "2023-09-21T18:20:21.745684Z"
    },
    "papermill": {
     "duration": 0.133425,
     "end_time": "2023-09-21T18:20:21.760559",
     "exception": false,
     "start_time": "2023-09-21T18:20:21.627134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "\n",
    "# # client = Client( n_workers=2, )\n",
    "# client = Client(n_workers=4, threads_per_worker=1, memory_limit='6GB',dashboard_address=':8087', timeout=\"600s\")\n",
    "\n",
    "# client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16e984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:21.847084Z",
     "iopub.status.busy": "2023-09-21T18:20:21.845838Z",
     "iopub.status.idle": "2023-09-21T18:20:21.860602Z",
     "shell.execute_reply": "2023-09-21T18:20:21.858603Z"
    },
    "papermill": {
     "duration": 0.06295,
     "end_time": "2023-09-21T18:20:21.868711",
     "exception": false,
     "start_time": "2023-09-21T18:20:21.805761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ade32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583b73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:21.961856Z",
     "iopub.status.busy": "2023-09-21T18:20:21.960116Z",
     "iopub.status.idle": "2023-09-21T18:20:21.972221Z",
     "shell.execute_reply": "2023-09-21T18:20:21.969674Z"
    },
    "papermill": {
     "duration": 0.064653,
     "end_time": "2023-09-21T18:20:21.979472",
     "exception": false,
     "start_time": "2023-09-21T18:20:21.914819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROCESSED_DATA = \"./code/processed_data/processed_final_data_latest.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c1f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:22.067563Z",
     "iopub.status.busy": "2023-09-21T18:20:22.066812Z",
     "iopub.status.idle": "2023-09-21T18:20:37.981314Z",
     "shell.execute_reply": "2023-09-21T18:20:37.977994Z"
    },
    "papermill": {
     "duration": 15.968299,
     "end_time": "2023-09-21T18:20:37.989959",
     "exception": false,
     "start_time": "2023-09-21T18:20:22.021660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(PROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b720337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf697fd-b1bc-4c98-bb4e-60acdba7dabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:38.115343Z",
     "iopub.status.busy": "2023-09-21T18:20:38.113553Z",
     "iopub.status.idle": "2023-09-21T18:20:40.071319Z",
     "shell.execute_reply": "2023-09-21T18:20:40.068461Z"
    },
    "papermill": {
     "duration": 2.030499,
     "end_time": "2023-09-21T18:20:40.078975",
     "exception": false,
     "start_time": "2023-09-21T18:20:38.048476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"code\")\n",
    "import os\n",
    "import feature_engineer as fe \n",
    "import data_processor as dp\n",
    "import config\n",
    "from helper import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    create_dirs,\n",
    "    create_classification_models,\n",
    "    create_logistic_regression_parameters,\n",
    "    create_random_forest_parameters,\n",
    "    create_xgb_classifier_parameters,\n",
    "    log_intermediate_output_to_file,\n",
    "    filter_dataset_by_date)\n",
    "import temporal_features as tmpf\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20445748-1d3e-4f28-bc73-80efbce7201c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:40.185955Z",
     "iopub.status.busy": "2023-09-21T18:20:40.184242Z",
     "iopub.status.idle": "2023-09-21T18:20:40.805204Z",
     "shell.execute_reply": "2023-09-21T18:20:40.800476Z"
    },
    "papermill": {
     "duration": 0.681388,
     "end_time": "2023-09-21T18:20:40.813519",
     "exception": false,
     "start_time": "2023-09-21T18:20:40.132131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dp.set_data_types_to_datetime(raw_data, [\"Project Posted Date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fd2fb-dfd4-4aa2-8112-e8587a757830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:40.929286Z",
     "iopub.status.busy": "2023-09-21T18:20:40.928229Z",
     "iopub.status.idle": "2023-09-21T18:20:41.508878Z",
     "shell.execute_reply": "2023-09-21T18:20:41.506015Z"
    },
    "papermill": {
     "duration": 0.650226,
     "end_time": "2023-09-21T18:20:41.522454",
     "exception": false,
     "start_time": "2023-09-21T18:20:40.872228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = filter_dataset_by_date(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e237fa-1d6a-4c06-82a2-1ae27488527c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:41.661302Z",
     "iopub.status.busy": "2023-09-21T18:20:41.659613Z",
     "iopub.status.idle": "2023-09-21T18:20:49.810936Z",
     "shell.execute_reply": "2023-09-21T18:20:49.808432Z"
    },
    "papermill": {
     "duration": 8.230897,
     "end_time": "2023-09-21T18:20:49.822043",
     "exception": false,
     "start_time": "2023-09-21T18:20:41.591146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_1 = dp.encode_data(data, config.CATEGORICAL_COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc46b27-d98d-4558-9e4f-33bc5b3389f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:20:49.941376Z",
     "iopub.status.busy": "2023-09-21T18:20:49.939795Z",
     "iopub.status.idle": "2023-09-21T18:21:07.122059Z",
     "shell.execute_reply": "2023-09-21T18:21:07.119921Z"
    },
    "papermill": {
     "duration": 17.248478,
     "end_time": "2023-09-21T18:21:07.126680",
     "exception": false,
     "start_time": "2023-09-21T18:20:49.878202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_folds = fe.split_data_folds(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74caac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:07.224229Z",
     "iopub.status.busy": "2023-09-21T18:21:07.222805Z",
     "iopub.status.idle": "2023-09-21T18:21:07.231481Z",
     "shell.execute_reply": "2023-09-21T18:21:07.229378Z"
    },
    "papermill": {
     "duration": 0.07422,
     "end_time": "2023-09-21T18:21:07.237949",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.163729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RuntimeError: No visualization engine detected, please install graphviz or ipycytoscape\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170163e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:07.348608Z",
     "iopub.status.busy": "2023-09-21T18:21:07.347606Z",
     "iopub.status.idle": "2023-09-21T18:21:07.365152Z",
     "shell.execute_reply": "2023-09-21T18:21:07.356006Z"
    },
    "papermill": {
     "duration": 0.084322,
     "end_time": "2023-09-21T18:21:07.373971",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.289649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dask constructs logic called task graph of your computationimmediately them only when necessary. this graph can be visualized as below\n",
    "# raw_data.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef34404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:07.487943Z",
     "iopub.status.busy": "2023-09-21T18:21:07.482418Z",
     "iopub.status.idle": "2023-09-21T18:21:07.495962Z",
     "shell.execute_reply": "2023-09-21T18:21:07.493316Z"
    },
    "papermill": {
     "duration": 0.07966,
     "end_time": "2023-09-21T18:21:07.502220",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.422560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sampled_raw_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbdcd5",
   "metadata": {
    "papermill": {
     "duration": 0.046825,
     "end_time": "2023-09-21T18:21:07.596934",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.550109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a623b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:07.692622Z",
     "iopub.status.busy": "2023-09-21T18:21:07.691523Z",
     "iopub.status.idle": "2023-09-21T18:21:07.701956Z",
     "shell.execute_reply": "2023-09-21T18:21:07.699981Z"
    },
    "papermill": {
     "duration": 0.069425,
     "end_time": "2023-09-21T18:21:07.710966",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.641541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered_data = raw_data[raw_data[\"Project ID\"].isin(sampled_raw_data[\"Project ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34912da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:07.800640Z",
     "iopub.status.busy": "2023-09-21T18:21:07.799331Z",
     "iopub.status.idle": "2023-09-21T18:21:07.807274Z",
     "shell.execute_reply": "2023-09-21T18:21:07.805647Z"
    },
    "papermill": {
     "duration": 0.049888,
     "end_time": "2023-09-21T18:21:07.811361",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.761473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d427276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:07.903242Z",
     "iopub.status.busy": "2023-09-21T18:21:07.901546Z",
     "iopub.status.idle": "2023-09-21T18:21:07.910651Z",
     "shell.execute_reply": "2023-09-21T18:21:07.908429Z"
    },
    "papermill": {
     "duration": 0.063375,
     "end_time": "2023-09-21T18:21:07.917724",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.854349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd805be",
   "metadata": {
    "papermill": {
     "duration": 0.050411,
     "end_time": "2023-09-21T18:21:08.021349",
     "exception": false,
     "start_time": "2023-09-21T18:21:07.970938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20969e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:08.104085Z",
     "iopub.status.busy": "2023-09-21T18:21:08.102744Z",
     "iopub.status.idle": "2023-09-21T18:21:08.110511Z",
     "shell.execute_reply": "2023-09-21T18:21:08.108704Z"
    },
    "papermill": {
     "duration": 0.05185,
     "end_time": "2023-09-21T18:21:08.115191",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.063341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Loading already processed data\")\n",
    "# # data = dp.load_data_to_df(path=data_file_path)\n",
    "# data = dp.set_data_types_to_datetime(filtered_data, [\"Project Posted Date\"])\n",
    "# data = filter_dataset_by_date(data)\n",
    "\n",
    "\n",
    "# data_folds = fe.split_data_folds(data_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dae40-32e8-403d-9018-724dbe3745ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:08.216663Z",
     "iopub.status.busy": "2023-09-21T18:21:08.214343Z",
     "iopub.status.idle": "2023-09-21T18:21:08.228025Z",
     "shell.execute_reply": "2023-09-21T18:21:08.225717Z"
    },
    "papermill": {
     "duration": 0.073342,
     "end_time": "2023-09-21T18:21:08.235275",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.161933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # from dask_ml.preprocessing import DummyEncoder\n",
    "# data = data.drop(columns=[\"School Name\"])\n",
    "# data_1 = dp.encode_data(data, config.CATEGORICAL_COLS)\n",
    "# # ddf= dd.from_pandas(data, npartitions=8)\n",
    "# # print(\"encoded_data.shape = \", data_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee9cd0-ab6b-4d3d-bdf4-b4d8ab38b871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:08.353126Z",
     "iopub.status.busy": "2023-09-21T18:21:08.352012Z",
     "iopub.status.idle": "2023-09-21T18:21:08.364391Z",
     "shell.execute_reply": "2023-09-21T18:21:08.360171Z"
    },
    "papermill": {
     "duration": 0.074966,
     "end_time": "2023-09-21T18:21:08.369831",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.294865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "# from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152dbd2-067b-44f7-8323-0320c7bb2608",
   "metadata": {
    "papermill": {
     "duration": 0.05473,
     "end_time": "2023-09-21T18:21:08.471759",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.417029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc230b-7012-41e6-9dd1-dedecb795b9e",
   "metadata": {
    "papermill": {
     "duration": 0.067645,
     "end_time": "2023-09-21T18:21:08.580770",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.513125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a099037-7f32-4839-b3e2-4db7e376bd24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:08.692570Z",
     "iopub.status.busy": "2023-09-21T18:21:08.690523Z",
     "iopub.status.idle": "2023-09-21T18:21:08.704139Z",
     "shell.execute_reply": "2023-09-21T18:21:08.701045Z"
    },
    "papermill": {
     "duration": 0.077865,
     "end_time": "2023-09-21T18:21:08.710501",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.632636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ddf = ddf.categorize(config.CATEGORICAL_COLS)\n",
    "\n",
    "# # Then generate an encoder. \n",
    "# encoder = DummyEncoder() \n",
    "\n",
    "# # Fit the encoder \n",
    "# ddf_enc = encoder.fit_transform(ddf[config.CATEGORICAL_COLS])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062c6ba-c0e2-400b-a9c8-1c6a86982dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:08.809051Z",
     "iopub.status.busy": "2023-09-21T18:21:08.808358Z",
     "iopub.status.idle": "2023-09-21T18:21:08.815296Z",
     "shell.execute_reply": "2023-09-21T18:21:08.813436Z"
    },
    "papermill": {
     "duration": 0.051712,
     "end_time": "2023-09-21T18:21:08.819067",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.767355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ddf_columns = [col for col in ddf.columns if col not in config.CATEGORICAL_COLS ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587bd76-7c43-4757-9a60-59cc3674db2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:08.911768Z",
     "iopub.status.busy": "2023-09-21T18:21:08.910166Z",
     "iopub.status.idle": "2023-09-21T18:21:08.925915Z",
     "shell.execute_reply": "2023-09-21T18:21:08.922479Z"
    },
    "papermill": {
     "duration": 0.082212,
     "end_time": "2023-09-21T18:21:08.931462",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.849250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ddf = dd.concat([ddf[ddf_columns], ddf_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc9227-598e-4303-b2d6-ea7fe65bff9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:09.043898Z",
     "iopub.status.busy": "2023-09-21T18:21:09.042913Z",
     "iopub.status.idle": "2023-09-21T18:21:09.055616Z",
     "shell.execute_reply": "2023-09-21T18:21:09.053260Z"
    },
    "papermill": {
     "duration": 0.074394,
     "end_time": "2023-09-21T18:21:09.061854",
     "exception": false,
     "start_time": "2023-09-21T18:21:08.987460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ddf.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa85fec-bfbb-462f-a981-270b3f421e03",
   "metadata": {
    "papermill": {
     "duration": 0.043686,
     "end_time": "2023-09-21T18:21:09.154651",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.110965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cf290-e124-4c2e-965e-3daa8818de7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:09.262474Z",
     "iopub.status.busy": "2023-09-21T18:21:09.261667Z",
     "iopub.status.idle": "2023-09-21T18:21:09.271311Z",
     "shell.execute_reply": "2023-09-21T18:21:09.269265Z"
    },
    "papermill": {
     "duration": 0.067155,
     "end_time": "2023-09-21T18:21:09.277122",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.209967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ddf_computed = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1044626-317f-4550-9584-bdf01bfaf360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:09.385107Z",
     "iopub.status.busy": "2023-09-21T18:21:09.383892Z",
     "iopub.status.idle": "2023-09-21T18:21:09.393320Z",
     "shell.execute_reply": "2023-09-21T18:21:09.391024Z"
    },
    "papermill": {
     "duration": 0.072134,
     "end_time": "2023-09-21T18:21:09.399635",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.327501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ddf_computed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5b049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:09.517534Z",
     "iopub.status.busy": "2023-09-21T18:21:09.516248Z",
     "iopub.status.idle": "2023-09-21T18:21:09.530561Z",
     "shell.execute_reply": "2023-09-21T18:21:09.528428Z"
    },
    "papermill": {
     "duration": 0.083299,
     "end_time": "2023-09-21T18:21:09.537177",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.453878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folds = fe.split_data_folds(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9c550-e4b2-49d9-834c-199a41fc549f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:09.637067Z",
     "iopub.status.busy": "2023-09-21T18:21:09.636312Z",
     "iopub.status.idle": "2023-09-21T18:21:09.659761Z",
     "shell.execute_reply": "2023-09-21T18:21:09.658179Z"
    },
    "papermill": {
     "duration": 0.075883,
     "end_time": "2023-09-21T18:21:09.667680",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.591797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initiate timing variables\n",
    "# def get_positive_percentage(y_train , y_test):\n",
    "#     train_pos = y_train[\"Label\"].value_counts()[1] / len(y_train[\"Label\"])\n",
    "#     test_pos = y_test[\"Label\"].value_counts()[1] / len(y_test[\"Label\"])\n",
    "\n",
    "#     return train_pos, test_pos\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "# import dask.delayed as delayed\n",
    "\n",
    "# from datetime import timedelta\n",
    "# max_t = pd.Timestamp(config.MAX_TIME)\n",
    "# min_t = pd.Timestamp(config.MIN_TIME)\n",
    "# shift_period = timedelta(days=config.LEAK_OFFSET)   # 4 months\n",
    "# fold_period = timedelta(days=config.WINDOW)    # 15 months\n",
    "\n",
    "# t_current = max_t\n",
    "# folds = 1\n",
    "\n",
    "# folded_dataset = []\n",
    "# # print(data.columns)\n",
    "\n",
    "# data = dd.from_pandas(ddf.compute(),npartitions=2)\n",
    "\n",
    "# while t_current > min_t + fold_period:\n",
    "#     start_date = t_current - fold_period\n",
    "\n",
    "#     delayed_result = delayed(dp.split_temporal_train_test_data(\n",
    "#         data=data,\n",
    "#         start_date=start_date)\n",
    "#     )\n",
    "    \n",
    "#     x_train, y_train, x_test, y_test = delayed_result.compute()\n",
    "    \n",
    "\n",
    "#     # Count the positive labeled percentage in the training set and the test set\n",
    "#     train_pos_perc, test_pos_perc = get_positive_percentage(\n",
    "#         y_train, y_test)\n",
    "\n",
    "#     fold_dataset = {\n",
    "#         \"fold_no\": folds,\n",
    "#         \"start_date\": start_date,\n",
    "#         \"x_train\": x_train,\n",
    "#         \"y_train\": y_train,\n",
    "#         \"x_test\": x_test,\n",
    "#         \"y_test\": y_test,\n",
    "#         \"train_pos_perc\": train_pos_perc,\n",
    "#         \"test_pos_perc\": test_pos_perc\n",
    "#     }\n",
    "#     folded_dataset.append(fold_dataset)\n",
    "\n",
    "#     train_end = start_date + timedelta(config.TRAIN_SIZE)\n",
    "#     test_start = train_end + timedelta(config.LEAK_OFFSET)\n",
    "#     test_end = test_start + timedelta(config.TEST_SIZE)\n",
    "\n",
    "#     fold_info = {\n",
    "#         'fold_number': folds,\n",
    "#         'timeline': {\n",
    "#             'train_start': str(start_date)[:10],\n",
    "#             'train_end': str(train_end)[:10],\n",
    "#             'test_start': str(test_start)[:10],\n",
    "#             'test_end': str(test_end)[:10]\n",
    "#         },\n",
    "#         'shape': {\n",
    "#             'training_shape': str(x_train.shape),\n",
    "#             'test_shape': str(x_test.shape),\n",
    "#         },\n",
    "#         'data_distribution': {\n",
    "#             'train_positive_ratio': train_pos_perc,\n",
    "#             'test_positive_ratio': test_pos_perc\n",
    "#         }\n",
    "#     }\n",
    "#     file_name = f\"Fold {folds} - {str(start_date)[:10]}.json\"\n",
    "#     dp.save_json(fold_info, config.INFO_DEST+file_name)\n",
    "\n",
    "#     # Combine x_train and y_train into a single DataFrame\n",
    "#     train_df_tmp = dd.concat([x_train, y_train], axis=1).compute()\n",
    "#     train_df = dd.concat(\n",
    "#      [data.loc[train_df_tmp.index][\"Project ID\"], train_df_tmp], axis=1)\n",
    "\n",
    "#     # Combine x_test, y_test, and predicted_probabilities into a single DataFrame\n",
    "#     test_df_tmp = dd.concat([x_test, y_test], axis=1).compute()\n",
    "#     test_df = dd.concat(\n",
    "#      [data.loc[test_df_tmp.index][\"Project ID\"], test_df_tmp], axis=1)\n",
    "\n",
    "#     art_path = config.ARTIFACTS_PATH\n",
    "#     train_df.to_csv(\n",
    "#     art_path + f'train_fold_{folds}_{str(start_date)[:10]}.csv', single_file=True\n",
    "#     )\n",
    "#     test_df.to_csv(\n",
    "#     art_path + f'test_fold_{folds}_{str(start_date)[:10]}.csv', single_file=True\n",
    "#     )\n",
    "#     log_intermediate_output_to_file(\n",
    "#     config.INFO_DEST, config.PROGRAM_LOG_FILE, f'Fold {folds} is done.')\n",
    "data_folds[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ba70b-cc62-49bf-9638-0f28fc5a3d14",
   "metadata": {
    "papermill": {
     "duration": 0.055759,
     "end_time": "2023-09-21T18:21:09.781339",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.725580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the necessary paths to obtain data sources after running the main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ddedc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:09.884040Z",
     "iopub.status.busy": "2023-09-21T18:21:09.883358Z",
     "iopub.status.idle": "2023-09-21T18:21:09.890415Z",
     "shell.execute_reply": "2023-09-21T18:21:09.888809Z"
    },
    "papermill": {
     "duration": 0.065023,
     "end_time": "2023-09-21T18:21:09.894052",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.829029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_test = \"/storage/scratch/e17-fyp-xai/projects/e17-4yp-using-machine-learning-in-high-stake-settings/code/model_outputs/artifacts/random_forest_t_300_md_10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123c404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.010079Z",
     "iopub.status.busy": "2023-09-21T18:21:10.008831Z",
     "iopub.status.idle": "2023-09-21T18:21:10.020891Z",
     "shell.execute_reply": "2023-09-21T18:21:10.018583Z"
    },
    "papermill": {
     "duration": 0.07388,
     "end_time": "2023-09-21T18:21:10.027316",
     "exception": false,
     "start_time": "2023-09-21T18:21:09.953436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_train = \"/storage/scratch/e17-4yp-xai/cepdnaclk/e17-4yp-using-machine-learning-in-high-stake-settings/code/model_outputs/info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeea3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.119589Z",
     "iopub.status.busy": "2023-09-21T18:21:10.118858Z",
     "iopub.status.idle": "2023-09-21T18:21:10.130953Z",
     "shell.execute_reply": "2023-09-21T18:21:10.129013Z"
    },
    "papermill": {
     "duration": 0.055856,
     "end_time": "2023-09-21T18:21:10.135793",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.079937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # train_files = sorted(['Fold 1 - 2016-01-07.json', 'Fold 4 - 2015-01-12.json',\n",
    "# 'Fold 2 - 2015-09-09.json', 'Fold 5 - 2014-09-14.json',\n",
    "# 'Fold 3 - 2015-05-12.json',  'Fold 6 - 2014-05-17.json'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b125f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.227880Z",
     "iopub.status.busy": "2023-09-21T18:21:10.226853Z",
     "iopub.status.idle": "2023-09-21T18:21:10.238244Z",
     "shell.execute_reply": "2023-09-21T18:21:10.235400Z"
    },
    "papermill": {
     "duration": 0.066272,
     "end_time": "2023-09-21T18:21:10.249829",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.183557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test_files = sorted([\"test_prediction_fold_1_2016-01-07.csv\",  \"test_prediction_fold_4_2015-01-12.csv\",\n",
    "# \"test_prediction_fold_2_2015-09-09.csv\",  \"test_prediction_fold_5_2014-09-14.csv\",\n",
    "# \"test_prediction_fold_3_2015-05-12.csv\",  \"test_prediction_fold_6_2014-05-17.csv\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742aee3-df73-486e-991b-395e73d7c940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.345122Z",
     "iopub.status.busy": "2023-09-21T18:21:10.344181Z",
     "iopub.status.idle": "2023-09-21T18:21:10.365668Z",
     "shell.execute_reply": "2023-09-21T18:21:10.363531Z"
    },
    "papermill": {
     "duration": 0.073953,
     "end_time": "2023-09-21T18:21:10.370069",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.296116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "directory = \"/storage/scratch/e17-fyp-xai/projects/e17-4yp-using-machine-learning-in-high-stake-settings/code/model_outputs/artifacts/random_forest_t_300_md_10\"\n",
    "\n",
    "\n",
    "pkl_files = glob.glob(f\"{directory}/*.pkl\")\n",
    "pkl_file_names = [os.path.basename(file) for file in pkl_files]\n",
    "models = sorted(pkl_file_names)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7e8f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.498065Z",
     "iopub.status.busy": "2023-09-21T18:21:10.490417Z",
     "iopub.status.idle": "2023-09-21T18:21:10.519116Z",
     "shell.execute_reply": "2023-09-21T18:21:10.516847Z"
    },
    "papermill": {
     "duration": 0.116559,
     "end_time": "2023-09-21T18:21:10.525734",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.409175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_files = glob.glob(f\"{directory}/*.csv\")\n",
    "csv_file_names = [os.path.basename(file) for file in csv_files]\n",
    "test_pred = sorted(csv_file_names)\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705e35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.652938Z",
     "iopub.status.busy": "2023-09-21T18:21:10.651446Z",
     "iopub.status.idle": "2023-09-21T18:21:10.676347Z",
     "shell.execute_reply": "2023-09-21T18:21:10.668461Z"
    },
    "papermill": {
     "duration": 0.101251,
     "end_time": "2023-09-21T18:21:10.684426",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.583175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"000\",os.path.exists(os.path.join(root_test, test_pred[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a691e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:10.852446Z",
     "iopub.status.busy": "2023-09-21T18:21:10.851457Z",
     "iopub.status.idle": "2023-09-21T18:21:10.907231Z",
     "shell.execute_reply": "2023-09-21T18:21:10.905099Z"
    },
    "papermill": {
     "duration": 0.166839,
     "end_time": "2023-09-21T18:21:10.913758",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.746919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### SET PATH\n",
    "# ROOT = \"/storage/scratch/e17-4yp-xai/Documents/e17-4yp-using-machine-learning-in-high-stake-settings/code/\"\n",
    "ROOT = \"./\"\n",
    "LIME_DEST = ROOT + \"model_outputs_5/lime/\"\n",
    "SHAP_DEST = ROOT + \"model_outputs_5/shap/\"\n",
    "TREESHAP_DEST = ROOT + \"model_outputs_5/treeshap/\"\n",
    "OUTPUT_DEST = ROOT + \"model_outputs_5/xai_results/\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(LIME_DEST,exist_ok=True)\n",
    "os.makedirs(SHAP_DEST,exist_ok=True)\n",
    "os.makedirs(TREESHAP_DEST,exist_ok=True)\n",
    "os.makedirs(OUTPUT_DEST,exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab06bb6-d334-458e-bb89-40f546f790e0",
   "metadata": {
    "papermill": {
     "duration": 0.06638,
     "end_time": "2023-09-21T18:21:11.032017",
     "exception": false,
     "start_time": "2023-09-21T18:21:10.965637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075a351-2ffd-4185-858c-66fcca46a057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:11.126947Z",
     "iopub.status.busy": "2023-09-21T18:21:11.126197Z",
     "iopub.status.idle": "2023-09-21T18:21:11.136625Z",
     "shell.execute_reply": "2023-09-21T18:21:11.134680Z"
    },
    "papermill": {
     "duration": 0.062916,
     "end_time": "2023-09-21T18:21:11.140371",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.077455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define paths for testing locally  - comment out when not required \n",
    "# root = 'C:/Users/User/random_forest/'\n",
    "# train_files = sorted(['train_fold_1_2017-01-06.csv'])\n",
    "# test_files = ['test_fold_1_2017-01-06.csv']\n",
    "# test_pred = ['test_prediction_fold_1_2017-01-06.csv']\n",
    "# models = sorted([\"random_forest_fold_1_2017-01-06.pkl\"])\n",
    "# LIME_DEST = root + \"xai_test_results/lime/\"\n",
    "# SHAP_DEST = root + \"xai_test_results/kernelshap/\"\n",
    "# TREESHAP_DEST = root + \"xai_test_results/treeshap/\"\n",
    "# OUTPUT_DEST = root + \"xai_test_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfaad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:11.264243Z",
     "iopub.status.busy": "2023-09-21T18:21:11.263428Z",
     "iopub.status.idle": "2023-09-21T18:21:11.276105Z",
     "shell.execute_reply": "2023-09-21T18:21:11.274341Z"
    },
    "papermill": {
     "duration": 0.090508,
     "end_time": "2023-09-21T18:21:11.280134",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.189626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required helper functions\n",
    "\n",
    "def load_model(model_file_path):\n",
    "    return pickle.load(open(model_file_path, 'rb'))\n",
    "\n",
    "def save_json(dict_obj: Union[dict, list], path: str):\n",
    "    writable_json = json.dumps(dict_obj, indent=4)\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(writable_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affab0d-791c-45dd-ac39-223604a21a09",
   "metadata": {
    "papermill": {
     "duration": 0.043582,
     "end_time": "2023-09-21T18:21:11.359215",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.315633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Functions for LIME, TreeSHAP and KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2a8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:11.477507Z",
     "iopub.status.busy": "2023-09-21T18:21:11.476704Z",
     "iopub.status.idle": "2023-09-21T18:21:11.524975Z",
     "shell.execute_reply": "2023-09-21T18:21:11.522757Z"
    },
    "papermill": {
     "duration": 0.117267,
     "end_time": "2023-09-21T18:21:11.531116",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.413849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Functions used to generate LIME explanation and to save the explanation\n",
    "\n",
    "# Function to convert the features in a LIME explanation object to a list of tuples\n",
    "def get_lime_feature_list(exp_object, feat_names):\n",
    "    exp_list = exp_object.as_map().get(1)\n",
    "    exp_list_with_feature_names = [(feat_names[x[0]], x[1]) for x in exp_list]\n",
    "    print(f'exp_list_with_feature_names = {exp_list_with_feature_names}')\n",
    "    return exp_list_with_feature_names\n",
    "\n",
    "def get_lime_feature_list_encoded(exp_object):\n",
    "    exp_list = exp_object.as_map().get(1)\n",
    "    print(f'exp_list = {exp_list}')\n",
    "    return exp_list\n",
    "\n",
    "def save_lime_explanation(exp, instance_loc, model_name, position, project_id,fold):\n",
    "\n",
    "    # exp.show_in_notebook(show_table=True)\n",
    "    # Save as html file\n",
    "    filepath = f\"{LIME_DEST}{fold}/{position}\"\n",
    "    try:\n",
    "        os.makedirs(filepath, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" %filepath)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "\n",
    "    # Saving the explanation as an image\n",
    "    exp.save_to_file(f'{LIME_DEST}{fold}/{position}/lime_exp_{project_id}_{model_name}.html')\n",
    "\n",
    "    # Save as pyplot figure\n",
    "    exp.as_pyplot_figure()\n",
    "    plt.savefig(f'{LIME_DEST}{fold}/{position}/lime_exp_{project_id}_{model_name}.png')\n",
    "    print(f\"Saving lime exp for {project_id}_{model_name}\")\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Function to generate explanations for a list of instances\n",
    "def get_lime_for_list(instance_loc_list, x_test, explainer_lime, model, model_name, list_type, fold, num_of_feat):\n",
    "\n",
    "    # Dictionary used to save the explanation objects\n",
    "    exp_objects = {}\n",
    "    exp_as_list_objects = {}\n",
    "    \n",
    "    for instance_loc in instance_loc_list:\n",
    "        # Select instance\n",
    "        instance = x_test.iloc[instance_loc]\n",
    "        # Find its Project ID\n",
    "        project_id = instance[\"Project ID\"]\n",
    "        # Drop the Project ID value from the instance since its not a feature\n",
    "        instance = instance.drop([\"Project ID\"])\n",
    "        # Get the explanation\n",
    "        exp = explainer_lime.explain_instance(\n",
    "            data_row=instance,\n",
    "            predict_fn=model.predict_proba,\n",
    "            num_features=num_of_feat\n",
    "        )\n",
    "        # Append exp object to dictionary\n",
    "        exp_objects[project_id] = exp\n",
    "        # Find the feature names as a list\n",
    "        feat_names = instance.keys().to_list()\n",
    "        # Get the explanation as a list of tuples and save\n",
    "        exp_as_list = get_lime_feature_list(exp, feat_names)\n",
    "        exp_as_list_objects[project_id] = exp_as_list\n",
    "        # Save the explanation as a figure\n",
    "        #save_lime_explanation(exp, instance_loc, model_name, list_type, project_id, fold)\n",
    "        \n",
    "    return exp_objects, exp_as_list_objects\n",
    "\n",
    "def get_lime_explanation(x_train, x_test, top_instance_loc_list, bottom_instance_loc_list, class_names, mode, model, model_name, fold, num_of_feat):\n",
    "\n",
    "    # take the list of instances and save the explaination of each instance.\n",
    "    # LIME: define the explainer\n",
    "    # Ex: mode = 'classification' or 'regression'\n",
    "    #     class_names = ['0', '1']\n",
    "\n",
    "    categorical_feature_names = x_train.dtypes[x_train.dtypes==bool].index.to_list()\n",
    "    categorical_feature_index = [x_train.columns.get_loc(col) for col in categorical_feature_names]\n",
    "    \n",
    "    # Define the explainer\n",
    "    explainer_lime = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=np.array(x_train),\n",
    "        feature_names=x_train.columns,\n",
    "        categorical_features = categorical_feature_index,\n",
    "        class_names=class_names,\n",
    "        mode=mode\n",
    "    )\n",
    "    \n",
    "    # Get LIME explanations for both top and bottom lists with Project ID\n",
    "    print(\"Top list\")\n",
    "    exp_objects_top, exp_as_list_objects_top = get_lime_for_list(top_instance_loc_list, x_test, explainer_lime, model, model_name, \"top\", fold, num_of_feat)\n",
    "    print(\"Bottom list\")\n",
    "    exp_objects_bottom, exp_as_list_objects_bottom = get_lime_for_list(bottom_instance_loc_list, x_test, explainer_lime, model, model_name, \"bottom\", fold, num_of_feat)\n",
    "    \n",
    "    return exp_as_list_objects_top, exp_as_list_objects_bottom\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ad73a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:11.641083Z",
     "iopub.status.busy": "2023-09-21T18:21:11.640173Z",
     "iopub.status.idle": "2023-09-21T18:21:11.685541Z",
     "shell.execute_reply": "2023-09-21T18:21:11.683772Z"
    },
    "papermill": {
     "duration": 0.107067,
     "end_time": "2023-09-21T18:21:11.691294",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.584227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_treeshap_explanation(explainer_tree, exp, instance, model_name, list_type, project_id, fold):\n",
    "\n",
    "    # Visualize and save\n",
    "    filepath = f'{TREESHAP_DEST}{fold}/{list_type}/treeshap_exp_{project_id}_{model_name}.png'\n",
    "    print(f\"Saving treeshap exp for {filepath}\")\n",
    "    shap.force_plot(explainer_tree.expected_value[1], \n",
    "                exp[1],\n",
    "                instance,\n",
    "                show=False, \n",
    "                matplotlib=True, \n",
    "                text_rotation=45).savefig(filepath, format = \"png\", dpi = 150, bbox_inches = 'tight')\n",
    "    return\n",
    "\n",
    "\n",
    "def get_treeshap_feature_list(exp, feat_names, num_of_feat):\n",
    "    \n",
    "    # Create list of tuples\n",
    "    shap_values = dict(zip(feat_names, exp[1][0]))\n",
    "    sorted_shap_exp = sorted(shap_values.items(), key=lambda x:abs(x[1]), reverse=True)[:num_of_feat]\n",
    "\n",
    "    #exp_list = exp[1].tolist()\n",
    "    #exp_list_with_feature_names = [(feat_names[x], exp_list[x]) for x in range(len(exp_list))]\n",
    "    #print(f'exp_list_with_feature_names = {exp_list_with_feature_names}')\n",
    "    \n",
    "    return sorted_shap_exp\n",
    "\n",
    "\n",
    "# Function to generate explanations for a list of instances\n",
    "def get_treeshap_for_list(instance_loc_list, x_test, explainer_tree, model_name, list_type, fold, num_of_feat):\n",
    "\n",
    "    # Dictionary used to save the explanation objects\n",
    "    exp_objects = {}\n",
    "    exp_as_list_objects = {}\n",
    "    \n",
    "    for i, instance_loc in enumerate(instance_loc_list):\n",
    "        # Select instance\n",
    "        instance = x_test.iloc[[instance_loc]]\n",
    "        # Find its Project ID\n",
    "        project_id = instance[\"Project ID\"].values[0]\n",
    "        # Drop the Project ID value from the instance since its not a feature\n",
    "        instance = instance.drop([\"Project ID\"], axis=1)\n",
    "        # Get the explanation\n",
    "        exp = explainer_tree.shap_values(instance)\n",
    "        #print(f'exp[1] = {exp[1]}')\n",
    "        # Append exp object to dictionary\n",
    "        exp_objects[project_id] = exp\n",
    "        # Find the feature names as a list\n",
    "        feat_names = instance.columns.to_list()\n",
    "        print(f'len(exp[1][0]) = {len(exp[1][0])}')\n",
    "        print(f'len(feat_names) = {len(feat_names)}')\n",
    "        print(f'exp[1][0] = {exp[1][0]}')\n",
    "        print(f'feat_names top {num_of_feat} = {feat_names[:num_of_feat]}')\n",
    "        \n",
    "        # Get the explanation as a list of tuples and save\n",
    "        exp_as_list = get_treeshap_feature_list(exp, feat_names, num_of_feat)\n",
    "        exp_as_list_objects[project_id] = exp_as_list\n",
    "        #print(exp_as_list)\n",
    "        # print(f'type(exp_as_list) = {type(exp_as_list)}')\n",
    "        # Save the explanation as a figure\n",
    "        save_treeshap_explanation(explainer_tree, exp, instance, model_name, list_type, project_id, fold)\n",
    "        \n",
    "    return exp_objects, exp_as_list_objects\n",
    "\n",
    "\n",
    "def get_treeshap_explanation(x_train, x_test, top_instance_loc_list, bottom_instance_loc_list, model, model_name, fold, num_of_feat):\n",
    "    \n",
    "#     print(x_train.head())\n",
    "    # Define the KernelSHAP explainer\n",
    "#     explainer_tree = shap.TreeExplainer(model=model, data=x_train, model_output=\"raw\")\n",
    "    print(f\"Treeshap explainer: Start training\")\n",
    "    explainer_tree = shap.TreeExplainer(model=model, feature_perturbation='interventional', data=x_train, model_output=\"raw\")\n",
    "    print(f\"Treeshap explainer: Done training\")\n",
    "    \n",
    "    filepath = f'{TREESHAP_DEST}{fold}/top/'\n",
    "    try:\n",
    "        os.makedirs(filepath, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" %filepath)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "        \n",
    "    filepath = f'{TREESHAP_DEST}{fold}/bottom/'\n",
    "    try:\n",
    "        os.makedirs(filepath, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" %filepath)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "            \n",
    "    print(\"Top list\")          \n",
    "    exp_objects_top, exp_as_list_objects_top = get_treeshap_for_list(top_instance_loc_list, x_test, explainer_tree, model_name, \"top\", fold, num_of_feat)\n",
    "    \n",
    "    print(\"Bottom list\")\n",
    "    exp_objects_bottom, exp_as_list_objects_bottom = get_treeshap_for_list(bottom_instance_loc_list, x_test, explainer_tree, model_name, \"bottom\", fold, num_of_feat)\n",
    "    \n",
    "    return exp_as_list_objects_top, exp_as_list_objects_bottom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadae52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:11.823977Z",
     "iopub.status.busy": "2023-09-21T18:21:11.819988Z",
     "iopub.status.idle": "2023-09-21T18:21:11.871395Z",
     "shell.execute_reply": "2023-09-21T18:21:11.869228Z"
    },
    "papermill": {
     "duration": 0.125007,
     "end_time": "2023-09-21T18:21:11.875552",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.750545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_kernelshap_explanation(explainer_tree, exp, instance, model_name, list_type, project_id, fold):\n",
    "\n",
    "    # Visualize and save\n",
    "    filepath = f'{SHAP_DEST}{fold}/{list_type}/kernelshap_exp_{project_id}_{model_name}.png'\n",
    "    print(f\"Saving kernelshap exp for {filepath}\")\n",
    "    shap.force_plot(explainer_tree.expected_value[1], \n",
    "                exp[1],\n",
    "                instance,\n",
    "                show=False, \n",
    "                matplotlib=True, \n",
    "                text_rotation=45).savefig(filepath, format = \"png\", dpi = 150, bbox_inches = 'tight')\n",
    "    return\n",
    "\n",
    "def get_kernelshap_feature_list(exp, feat_names, num_of_feat):\n",
    "    \n",
    "    # Create list of tuples\n",
    "    shap_values = dict(zip(feat_names, exp[1]))\n",
    "    sorted_shap_exp = sorted(shap_values.items(), key=lambda x:abs(x[1]), reverse=True)[:num_of_feat]\n",
    "\n",
    "    #exp_list = exp[1].tolist()\n",
    "    #exp_list_with_feature_names = [(feat_names[x], exp_list[x]) for x in range(len(exp_list))]\n",
    "    #print(f'exp_list_with_feature_names = {exp_list_with_feature_names}')\n",
    "    \n",
    "    return sorted_shap_exp\n",
    "\n",
    "\n",
    "# Function to generate explanations for a list of instances\n",
    "def get_kernelshap_for_list(instance_loc_list, x_test, explainer_tree, model_name, list_type, fold, num_of_feat, nsamples):\n",
    "\n",
    "    # Dictionary used to save the explanation objects\n",
    "    exp_objects = {}\n",
    "    exp_as_list_objects = {}\n",
    "    \n",
    "    for i, instance_loc in enumerate(instance_loc_list):\n",
    "        # Select instance\n",
    "        instance = x_test.iloc[instance_loc]\n",
    "        # Find its Project ID\n",
    "        project_id = instance[\"Project ID\"]\n",
    "        # Drop the Project ID value from the instance since its not a feature\n",
    "        instance = instance.drop([\"Project ID\"])\n",
    "        \n",
    "        # Get the explanation\n",
    "        exp = explainer_tree.shap_values(instance, nsamples=nsamples) # nsamples can be either 'auto' or an int\n",
    "        print(f'len(exp[1]) = {len(exp[1])}')\n",
    "        print(f'exp[1] = {exp[1]}')\n",
    "        # Find the feature names as a list\n",
    "        feat_names = instance.keys().to_list()\n",
    "        print(f'len(feat_names) = {len(feat_names)}')\n",
    "        print(f'feat_names top entries = {feat_names[:num_of_feat]}')\n",
    "        \n",
    "        # Append exp object to dictionary\n",
    "        exp_objects[project_id] = exp\n",
    "        # Get the explanation as a list of tuples and save\n",
    "        exp_as_list = get_kernelshap_feature_list(exp, feat_names, num_of_feat)\n",
    "        exp_as_list_objects[project_id] = exp_as_list\n",
    "        \n",
    "        # print(f'type(exp_as_list) = {type(exp_as_list)}')\n",
    "        # Save the explanation as a figure\n",
    "        #save_kernelshap_explanation(explainer_tree, exp, instance, model_name, list_type, project_id, fold)\n",
    "        \n",
    "    return exp_objects, exp_as_list_objects\n",
    "\n",
    "\n",
    "\n",
    "def get_kernelshap_explanation(x_train, x_test, top_instance_loc_list, bottom_instance_loc_list, model, model_name, fold, num_of_feat, nsamples):\n",
    "    \n",
    "    # Define the KernelSHAP explainer\n",
    "    print(\"Kernel Explainer Loading ..... \")\n",
    "    explainer_shap = shap.KernelExplainer(model=model.predict_proba, data=x_train)\n",
    "    print(\"Kernel Explainer : Done training\")\n",
    "    \n",
    "    filepath = f'{SHAP_DEST}{fold}/top/'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(filepath, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" %filepath)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "        \n",
    "    filepath = f'{SHAP_DEST}{fold}/bottom/'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(filepath, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" %filepath)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "            \n",
    "            \n",
    "    print(\"Top list\")    \n",
    "    exp_objects_top, exp_as_list_objects_top = get_kernelshap_for_list(top_instance_loc_list, x_test, explainer_shap, model_name, \"top\", fold, num_of_feat, nsamples)   \n",
    "    print(\"Bottom list\")\n",
    "    exp_objects_bottom, exp_as_list_objects_bottom = get_kernelshap_for_list(bottom_instance_loc_list, x_test, explainer_shap, model_name, \"bottom\", fold, num_of_feat, nsamples)\n",
    "\n",
    "    return exp_as_list_objects_top, exp_as_list_objects_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68092194-9209-4277-ad7e-fb09d961cc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:11.998307Z",
     "iopub.status.busy": "2023-09-21T18:21:11.997505Z",
     "iopub.status.idle": "2023-09-21T18:21:12.031617Z",
     "shell.execute_reply": "2023-09-21T18:21:12.029700Z"
    },
    "papermill": {
     "duration": 0.103826,
     "end_time": "2023-09-21T18:21:12.038186",
     "exception": false,
     "start_time": "2023-09-21T18:21:11.934360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function used to create csv using explanations\n",
    "def create_csv_for_exps(feat_names, lime_dict, treeshap_dict, kernelshap_dict, list_type, fold):\n",
    "    # First create an empty df\n",
    "    data = pd.DataFrame()\n",
    "    # Create feature column\n",
    "    data['features'] = feat_names\n",
    "    # Create columns to store lime, treeshap and kernalshap values for each project in the dictionary\n",
    "    projects_list = list(lime_dict.keys())\n",
    "    print(f'Projects list = {projects_list}')\n",
    "    for project in projects_list:\n",
    "        # Create empty columns\n",
    "        data[f'lime_{list_type}_{project}'] = \"\" \n",
    "        data[f'treeshap_{list_type}_{project}'] = \"\"\n",
    "        data[f'kernelshap_{list_type}_{project}'] = \"\"\n",
    "\n",
    "        # Get lime, treeshap and kernelshap lists of tuples\n",
    "        lime_list = lime_dict[project]\n",
    "        treeshap_list = treeshap_dict[project]\n",
    "        kernelshap_list = kernelshap_dict[project]\n",
    "\n",
    "        # Fill the necessary cells with data\n",
    "        # For lime\n",
    "        for item in lime_list:\n",
    "            feature = item[0]\n",
    "            value = item[1]\n",
    "            data.loc[data['features'] == feature, f'lime_{list_type}_{project}'] = value\n",
    "        # For treeshap\n",
    "        for item in treeshap_list:\n",
    "            feature = item[0]\n",
    "            value = item[1]\n",
    "            data.loc[data['features'] == feature, f'treeshap_{list_type}_{project}'] = value\n",
    "        # For kernelshap\n",
    "        for item in kernelshap_list:\n",
    "            feature = item[0]\n",
    "            value = item[1]\n",
    "            data.loc[data['features'] == feature, f'kernelshap_{list_type}_{project}'] = value\n",
    "\n",
    "    \n",
    "    # print(data.columns)\n",
    "    # Save as csv\n",
    "    data.to_csv(OUTPUT_DEST+f'all_explanations_{fold}_{list_type}.csv') #### MAKE SURE OUTPUT_DEST IS DEFINED\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2884c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:12.158977Z",
     "iopub.status.busy": "2023-09-21T18:21:12.158225Z",
     "iopub.status.idle": "2023-09-21T18:21:12.214476Z",
     "shell.execute_reply": "2023-09-21T18:21:12.212400Z"
    },
    "papermill": {
     "duration": 0.121155,
     "end_time": "2023-09-21T18:21:12.219206",
     "exception": false,
     "start_time": "2023-09-21T18:21:12.098051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New pipeline for running the explainations - call this from main\n",
    "def explanations_pipeline(root, model_paths, train_paths, test_paths, pred_paths, model_name):\n",
    "  \"\"\"this pipeline will generate explanations for the given model paths\n",
    "\n",
    "  Args:\n",
    "      root (_type_): This is the root path of the model artifacts\n",
    "      model_paths (_type_): these are the paths to the model artifacts\n",
    "      train_paths (_type_): these are the paths to the train artifacts\n",
    "      test_paths (_type_): these are the paths to the test artifacts\n",
    "      pred_paths (_type_): these are the paths to the prediction artifacts\n",
    "      model_name (str): this is the model name as a string for identification. ex: \"random forest\"\n",
    "      \n",
    "      all the artifacts should be a list of files and should be in the same order\n",
    "  \"\"\"\n",
    "  assert len(train_paths) == len(test_paths), \"There should be same number of train paths and test paths\"\n",
    "  assert len(test_paths) == len(pred_paths), \"There should be same number of predictions paths and test paths\"\n",
    "  assert len(model_paths) == len(test_paths), \"There should be same number of model_paths paths and test paths\"\n",
    "\n",
    "  num_of_explanations = 1 # Should be 50 #### CHANGE THE VALUE TO 50\n",
    "  num_of_records = 20 # Should be 1000 #### CHANGE THE VALUE TO 1000\n",
    "  rows = 1000 # Should be None #### CHANGE THE VALUE TO NONE\n",
    "\n",
    "  # Dictionary to store all explanations\n",
    "  all_exp_dict = {}\n",
    "\n",
    "  # Loop to generate explanations for samples from each fold\n",
    "  for i in range(len(train_paths)):\n",
    "    print(f\"Fold {i} .............................\")\n",
    "    x_train = pd.read_csv(os.path.join(root, train_paths[i]), nrows=rows)\n",
    "    x_train_cleaned = x_train.drop([\"Unnamed: 0\", \"Project ID\", \"Label\"], axis=1)\n",
    "\n",
    "    fold1 = pd.read_csv(os.path.join(root,test_paths[i]), nrows=rows)\n",
    "    fold_pred =  pd.read_csv(os.path.join(root,  pred_paths[i]), nrows=rows)\n",
    "    Fold1 = pd.concat([fold1, fold_pred[\"1\"]],axis=1)\n",
    "    Fold1 = Fold1.drop([\"Unnamed: 0\"],axis=1)\n",
    "    Fold1_sort = Fold1.sort_values([\"1\"], ascending=False)\n",
    "    #Fold1_sort.head()\n",
    "    x_test_with_id = Fold1_sort.drop([ \"Label\", \"1\"],axis=1)\n",
    "\n",
    "    # Select n samples each from top and bottom k records\n",
    "    print(f\"Sampling the top {num_of_explanations} and bottom {num_of_explanations}\")\n",
    "    top_instance_loc_list = random.sample(range(num_of_records), num_of_explanations)\n",
    "    bottom_instance_loc_list = random.sample(range(x_test_with_id.shape[0]-num_of_records , x_test_with_id.shape[0]), num_of_explanations)\n",
    "    print(f'top_instance_loc_list = {top_instance_loc_list}')\n",
    "    print(f'bottom_instance_loc_list = {bottom_instance_loc_list}')\n",
    "      \n",
    "    # Define the number of features required to display after generating the explanations\n",
    "    num_of_feat = x_test_with_id.shape[1] - 1\n",
    "    #num_of_feat = 10 #### COMMENT THIS UNCOMMENT THE PREVIOUS LINE\n",
    "    x_train_n_rows = 1000 #### CHANGE IF NECESSARY\n",
    "\n",
    "    print(f\"Model {model_paths} is loading\")\n",
    "\n",
    "    # Load the saved model\n",
    "    model = load_model(os.path.join(root, model_paths[i]))\n",
    "\n",
    "    # Get explanations\n",
    "    print(f\"Explanation for Lime\")\n",
    "    lime_list_objects_top, lime_list_objects_bottom = get_lime_explanation(x_train_cleaned[:x_train_n_rows].astype(\"float64\"), x_test_with_id, top_instance_loc_list, bottom_instance_loc_list, [\"0\", \"1\"], \"classification\", model, model_name, f\"Fold{i}\", num_of_feat)\n",
    "    print(f\"Explanation for Kernel Shap\" )\n",
    "    kernelshap_list_objects_top, kernelshap_list_objects_bottom = get_kernelshap_explanation(x_train_cleaned[:x_train_n_rows].astype(\"float64\"), x_test_with_id, top_instance_loc_list, bottom_instance_loc_list, model, model_name, f\"Fold{i}\", num_of_feat, \"auto\") \n",
    "    print(f\"Explanation for Tree Shap\")\n",
    "    treeshap_list_objects_top, treeshap_list_objects_bottom = get_treeshap_explanation(x_train_cleaned[:x_train_n_rows].astype(\"float64\"), x_test_with_id, top_instance_loc_list, bottom_instance_loc_list, model, model_name, f\"Fold{i}\", num_of_feat) \n",
    "\n",
    "    #print(lime_list_objects_top)\n",
    "    #print(treeshap_list_objects_top)\n",
    "    #print(kernelshap_list_objects_top)\n",
    "\n",
    "    # Create dictionaries to store the explanations as json\n",
    "    current_fold_dict = {\n",
    "        'lime': {\n",
    "            'top': lime_list_objects_top,\n",
    "            'bottom': lime_list_objects_bottom\n",
    "        },\n",
    "        'treeshap': {\n",
    "            'top': treeshap_list_objects_top,\n",
    "            'bottom': treeshap_list_objects_bottom\n",
    "        },\n",
    "        'kernelshap': {\n",
    "            'top': kernelshap_list_objects_top,\n",
    "            'bottom': kernelshap_list_objects_bottom\n",
    "        }\n",
    "    }\n",
    "    all_exp_dict[f\"fold{i}\"] = current_fold_dict\n",
    "\n",
    "  # Get a list of all the features\n",
    "  list_of_features = x_test_with_id.drop([\"Project ID\"],axis=1).columns.to_list()\n",
    "  # Storing explanations as json\n",
    "  save_json(all_exp_dict, OUTPUT_DEST+'all_exp.json') #### MAKE SURE OUTPUT_DEST IS DEFINED\n",
    "\n",
    "  # Creating top and bottom csvs for each fold to summarize explanations\n",
    "  for i in range(len(train_paths)):\n",
    "      # For the top list\n",
    "      top_df = create_csv_for_exps(list_of_features, all_exp_dict[f\"fold{i}\"][\"lime\"][\"top\"], all_exp_dict[f\"fold{i}\"][\"treeshap\"][\"top\"], all_exp_dict[f\"fold{i}\"][\"kernelshap\"][\"top\"], \"top\", f\"fold{i}\")\n",
    "      # For the bottom list\n",
    "      bottom_df = create_csv_for_exps(list_of_features, all_exp_dict[f\"fold{i}\"][\"lime\"][\"bottom\"], all_exp_dict[f\"fold{i}\"][\"treeshap\"][\"bottom\"], all_exp_dict[f\"fold{i}\"][\"kernelshap\"][\"bottom\"], \"bottom\", f\"fold{i}\")\n",
    "    \n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c603e-07af-494c-8b74-38d2349363f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:12.345099Z",
     "iopub.status.busy": "2023-09-21T18:21:12.344376Z",
     "iopub.status.idle": "2023-09-21T18:21:12.401179Z",
     "shell.execute_reply": "2023-09-21T18:21:12.399470Z"
    },
    "papermill": {
     "duration": 0.118381,
     "end_time": "2023-09-21T18:21:12.405009",
     "exception": false,
     "start_time": "2023-09-21T18:21:12.286628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New pipeline for running the explainations - call this from main\n",
    "def explanations_pipeline_without_from_saving_files( root_test, model_paths, data_folds,  pred_paths, model_name):\n",
    "  \"\"\"this pipeline will generate explanations for the given model paths\n",
    "\n",
    "  Args:\n",
    "      root (_type_): This is the root path of the model artifacts\n",
    "      model_paths (_type_): these are the paths to the model artifacts\n",
    "      train_paths (_type_): these are the paths to the train artifacts\n",
    "      test_paths (_type_): these are the paths to the test artifacts\n",
    "      pred_paths (_type_): these are the paths to the prediction artifacts\n",
    "      model_name (str): this is the model name as a string for identification. ex: \"random forest\"\n",
    "      \n",
    "      all the artifacts should be a list of files and should be in the same order\n",
    "  \"\"\"\n",
    "  # assert len(train_paths) == len(test_paths), \"There should be same number of train paths and test paths\"\n",
    "  # assert len(test_paths) == len(pred_paths), \"There should be same number of predictions paths and test paths\"\n",
    "  # assert len(model_paths) == len(test_paths), \"There should be same number of model_paths paths and test paths\"\n",
    "\n",
    "  num_of_explanations = 500 # Should be 50 #### CHANGE THE VALUE TO 50\n",
    "  num_of_records = 1000 # Should be 1000 #### CHANGE THE VALUE TO 1000\n",
    "  rows = None # Should be None #### CHANGE THE VALUE TO NONE\n",
    "\n",
    "  # Dictionary to store all explanations\n",
    "#   all_exp_dict = {}\n",
    "\n",
    "  output_dest = OUTPUT_DEST  # Ensure OUTPUT_DEST is defined\n",
    "  output_file = f'{output_dest}all_exp.json'\n",
    "\n",
    "  # Loop to generate explanations for samples from each fold\n",
    "  for i in range(len(data_folds)-1, -1, -1):\n",
    "    print(i)\n",
    "    print(f\"Fold {i} .............................\")\n",
    "\n",
    "    # print(\"Data Fold\", data_folds[i].shape)\n",
    "      \n",
    "    if rows:\n",
    "        x_train = data_folds[i][\"x_train\"][:rows]\n",
    "    else:\n",
    "        x_train = data_folds[i][\"x_train\"]\n",
    "    x_train =  x_train.dropna()\n",
    "    x_train_cleaned = x_train.drop([\"Project ID\"], axis=1).reset_index(drop=True)\n",
    "\n",
    "    print(\"Train Fold\", x_train_cleaned.shape)\n",
    "\n",
    "    if rows:\n",
    "        fold1 = data_folds[i][\"x_test\"][:rows]\n",
    "    else:\n",
    "        fold1 = data_folds[i][\"x_test\"]\n",
    "\n",
    "    # fold1 = pd.read_csv(os.path.join(root,test_paths[i]), nrows=rows)\n",
    "    fold1 = fold1.reset_index(drop=True)\n",
    "    print(\"test fold\", fold1.shape)\n",
    "    # print(\"test fold list\", fold1)\n",
    "      \n",
    "    # print(fold1[fold1==NaN])\n",
    "    \n",
    "\n",
    "    fold_pred =  pd.read_csv(os.path.join(root_test,  pred_paths[i]))\n",
    "\n",
    "    fold_pred = fold_pred.drop([\"Unnamed: 0\"],axis=1).reset_index(drop=True)\n",
    "    print(\"prediction fold \", fold_pred.shape)\n",
    "\n",
    "    # print(\"prediction fold list\", fold_pred)\n",
    "\n",
    "      \n",
    "    Fold1 = pd.concat([fold1, fold_pred[\"1\"]],axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"concatenated\", Fold1.shape)\n",
    "    # Fold1 = Fold1.drop([\"Unnamed: 0\"],axis=1)\n",
    "    Fold1_sort = Fold1.sort_values([\"1\"], ascending=False)\n",
    "    print(Fold1_sort.head())\n",
    "    x_test_with_id = Fold1_sort.drop([\"1\"],axis=1)\n",
    "    print(x_test_with_id.head())\n",
    "    \n",
    "\n",
    "    # Select n samples each from top and bottom k records\n",
    "    print(f\"Sampling the top {num_of_explanations} and bottom {num_of_explanations}\")\n",
    "    # top_instance_loc_list = random.sample(range(num_of_records), num_of_explanations)\n",
    "    top_instance_loc_list = range(num_of_explanations)\n",
    "    bottom_instance_loc_list = range(x_test_with_id.shape[0]-num_of_explanations -1, x_test_with_id.shape[0])\n",
    "    # bottom_instance_loc_list = random.sample(range(x_test_with_id.shape[0]-num_of_records , x_test_with_id.shape[0]), num_of_explanations)\n",
    "\n",
    "    # bottom_instance_loc_list\n",
    "      \n",
    "    print(f'top_instance_loc_list = {top_instance_loc_list}')\n",
    "    print(f'bottom_instance_loc_list = {bottom_instance_loc_list}')\n",
    "      \n",
    "    # Define the number of features required to display after generating the explanations\n",
    "    num_of_feat = x_test_with_id.shape[1] - 1\n",
    "    #num_of_feat = 10 #### COMMENT THIS UNCOMMENT THE PREVIOUS LINE\n",
    "    x_train_n_rows = 1000 #### CHANGE IF NECESSARY\n",
    "\n",
    "    print(f\"Model {model_paths} is loading\")\n",
    "      \n",
    "\n",
    "    # Load the saved model\n",
    "    model = load_model(os.path.join(root_test, model_paths[i]))\n",
    "\n",
    "\n",
    "    # Get explanations\n",
    "    print(f\"Explanation for Lime\")\n",
    "    lime_list_objects_top, lime_list_objects_bottom = get_lime_explanation(x_train_cleaned.astype(\"float64\"), x_test_with_id, top_instance_loc_list, bottom_instance_loc_list, [\"0\", \"1\"], \"classification\", model, model_name, f\"Fold{i}\", num_of_feat)\n",
    "    print(f\"Explanation for Kernel Shap\" )\n",
    "    #kernelshap_list_objects_top, kernelshap_list_objects_bottom = get_kernelshap_explanation(x_train_cleaned[:x_train_n_rows].astype(\"float64\"), x_test_with_id, top_instance_loc_list, bottom_instance_loc_list, model, model_name, f\"Fold{i}\", num_of_feat, 'auto') \n",
    "    print(f\"Explanation for Tree Shap\")\n",
    "    treeshap_list_objects_top, treeshap_list_objects_bottom = get_treeshap_explanation(x_train_cleaned.astype(\"float64\"), x_test_with_id, top_instance_loc_list, bottom_instance_loc_list, model, model_name, f\"Fold{i}\", num_of_feat) \n",
    "\n",
    "    #print(lime_list_objects_top)\n",
    "    #print(treeshap_list_objects_top)\n",
    "    #print(kernelshap_list_objects_top)\n",
    "\n",
    "    # Create dictionaries to store the explanations as json\n",
    "    current_fold_dict = {\n",
    "        'lime': {\n",
    "            'top': lime_list_objects_top,\n",
    "            'bottom': lime_list_objects_bottom\n",
    "        },\n",
    "        'treeshap': {\n",
    "            'top': treeshap_list_objects_top,\n",
    "            'bottom': treeshap_list_objects_bottom\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(output_dest,exist_ok=True)\n",
    "    \n",
    "    # Load the existing JSON data or initialize an empty dictionary if the file is empty\n",
    "    try:\n",
    "        with open(output_file, 'r') as json_file:\n",
    "            all_exp_dict = json.load(json_file)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        all_exp_dict = {}\n",
    "\n",
    "    # Add the current fold data to the existing dictionary\n",
    "    all_exp_dict[f\"fold{i}\"] = current_fold_dict\n",
    "\n",
    "    # Save the updated dictionary back to the JSON file\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(all_exp_dict, json_file, indent=4)\n",
    "        \n",
    "    \n",
    "        \n",
    "#     all_exp_dict[f\"fold{i}\"] = current_fold_dict\n",
    "#     save_json(all_exp_dict, OUTPUT_DEST+'all_exp.json') #### MAKE SURE OUTPUT_DEST IS DEFINED\n",
    "    # return\n",
    "    # 'kernelshap': {\n",
    "    #         'top': kernelshap_list_objects_top,\n",
    "    #         'bottom': kernelshap_list_objects_bottom\n",
    "    #     }\n",
    "\n",
    "  \n",
    "\n",
    "  # Get a list of all the features\n",
    "  list_of_features = x_test_with_id.drop([\"Project ID\"],axis=1).columns.to_list()\n",
    "  # Storing explanations as json\n",
    "#   save_json(all_exp_dict, OUTPUT_DEST+'all_exp.json') #### MAKE SURE OUTPUT_DEST IS DEFINED\n",
    "  with open(output_file, 'r') as json_file:\n",
    "            all_exp_dict = json.load(json_file)\n",
    "\n",
    "  # Creating top and bottom csvs for each fold to summarize explanations\n",
    "  for i in range(6):\n",
    "      # For the top list\n",
    "      top_df = create_csv_for_exps(list_of_features, all_exp_dict[f\"fold{i}\"][\"lime\"][\"top\"], all_exp_dict[f\"fold{i}\"][\"treeshap\"][\"top\"], all_exp_dict[f\"fold{i}\"][\"kernelshap\"][\"top\"], \"top\", f\"fold{i}\")\n",
    "      # For the bottom list\n",
    "      bottom_df = create_csv_for_exps(list_of_features, all_exp_dict[f\"fold{i}\"][\"lime\"][\"bottom\"], all_exp_dict[f\"fold{i}\"][\"treeshap\"][\"bottom\"], all_exp_dict[f\"fold{i}\"][\"kernelshap\"][\"bottom\"], \"bottom\", f\"fold{i}\")\n",
    "  # exit(0)\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f75d25-d953-4cb8-80f5-a7d5a19ce350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T18:21:12.539782Z",
     "iopub.status.busy": "2023-09-21T18:21:12.538997Z"
    },
    "papermill": {
     "duration": 19209.300651,
     "end_time": "2023-09-21T23:41:21.778331",
     "exception": false,
     "start_time": "2023-09-21T18:21:12.477680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanations_pipeline_without_from_saving_files(directory, models, data_folds, test_pred, \"random_forest_300_md_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83282e-b9b4-457d-9091-5e5d910be2c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f472d3d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# explanations_pipeline(root, models, train_files, test_files, test_pred, \"random_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2a72b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf99678-7229-4590-a676-4190e1ec599c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0712e55-59db-4c60-afef-d02baa9b2bc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19322.483123,
   "end_time": "2023-09-21T23:41:55.417793",
   "environment_variables": {},
   "exception": null,
   "input_path": "XAI_Pipeline.ipynb",
   "output_path": "XAI_Pipeline_sep_22_top_500_bottom_500.ipynb",
   "parameters": {},
   "start_time": "2023-09-21T18:19:52.934670",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
