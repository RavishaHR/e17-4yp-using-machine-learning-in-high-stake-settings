___
# Using Machine Learning in High-stakes settings: Ensuring Transparency and Equity
___

## First steps on the domain

Explainable ML tools aim to provide transparency and interpretability in machine learning models. However, disagreements can arise in explanations due to different techniques, the complexity of models, and subjective interpretations. Addressing these challenges is crucial for building trust and promoting responsible AI practices.

There are certain instances in the real-world, where ML models need to explain the reasoning behind their outcomes and decisions. Such models, where explainability is crucial, can be encountered in healthcare, financial, and justice systems etc. due to the strong nature of the impact made by these systems in the real-world. These systems usually operate in resource constrained environments, which makes the decisions within their domain quite critical in nature. The decisions made by the ML models utilized in such environments require constant domain expertise to further refine the output in order to arrive at the best possible decision. The outcome expected by the models and the reasoning behind them in different environments can take different forms depending on the nature of the affected individuals and available resources. ML models taking decisions in these settings require to have their decisions explained to identify possible existing biases, proof of non-discrimination, to identify counterfactuals, and due to safety concerns. Due to the nature of different ML models and the impact they could have on society, it is crucial to understand the reasoning behind the model decisions and the path taken by the model to arrive at these decisions. This can guide the relevant decision makers and action takers to either accept or reject the decision of the model based on the feature-importance factors used by the model and existing biases in the model. Additionally, developing explanations for ML models allows us to perform model debugging, which can in turn help us to improve the inner workings of the model while eliminating biases.

Explainable ML can be approached in two main ways; by building inherently interpretable models and through post hoc explanations explaining the model decisions after running. Inherently interpretable models are designed with transparency and explainability in mind while enabling the explanations for their decisions. Some such common approaches include rule-based models, risk scores, generalized additive models (GAM), prototype-based models, attention-based models etc. Post hoc explanation methods either provide their explanations locally or globally. Local explanations focus on explaining individual predictions by examining specific aspects of the model's decision-making process. They include techniques such as feature importances, rule-based methods, saliency maps, prototypes-based approaches, and counterfactual explanations. On the other hand, global explanations aim to explain the overall behavior of the model, providing a comprehensive understanding of how the model functions as a whole. They often involve aggregating or summarizing local explanations to provide a broader perspective. Global explanation techniques include collections of local explanations, representation-based methods, model distillation, and summaries of counterfactual explanations. Both types of explanations are valuable in different contexts and contribute to the interpretability and transparency of machine learning models.


As these post hoc methods are used in high stake settings, it is crucial to ensure the reliability of the explanations they generate. Various evaluation metrics have been proposed to assess the faithfulness and accuracy of explanations in mimicking the underlying modelâ€™s behavior. However, one limitation of these metrics is that they may not be universally applicable to all model classes and real-world scenarios. Prior work has emphasized the use of these metrics to analyze the behavior of post hoc explanations and identify their vulnerabilities, revealing that methods like LIME and SHAP can produce inconsistent, unstable, and vulnerable explanations that are susceptible to adversarial attacks and fair washing. In practice, DS/ML practitioners often employ multiple explanation methods concurrently to understand model predictions. However, there could be instances where methods produce conflicting explanations, such as different rankings of the most important features. In these situations, practitioners need to approach the issue carefully, as relying on misleading explanations could potentially lead to catastrophic situations.

In a study conducted by Harvard University, the researchers carried out semi-structured interviews with 25 data scientists experienced in explainability tools, gathering insights to formalize the concept of disagreement. They proposed an evaluation framework to quantitatively measure disagreement in model predictions.In their evaluation framework, they considered two aspects to measure disagreement: disagreement with respect to the top-k features and disagreement with respect to features of interest. For the first approach, they introduced metrics such as feature agreement, rank agreement, sign agreement, and signed rank agreement. Additionally, for the second approach, they proposed metrics like rank correlation and pairwise correlation. This aforementioned framework was used to analyze real data, involving four datasets, six state-of-the-art explanation methods, and six popular models. Additionally, an online user study with data scientists was conducted, presenting pairs of explanations that disagreed and gathering survey responses on which explanation they would rely on and why. The interviews and studies indicate that state of the art explanations methods often disagree in terms of the explanations they output. Also the empirical study confirmed that the explanations generated by the state-of-of-the-art explanations often disagree with each other and this phenomenon persists across various model classes and data modalities. 

Studies show that addressing the problem of explanation disagreement in machine learning still requires systematic study of its causes, proposing novel approaches to address this problem, rethinking the problem of explanation from scratch, and regularly educating data scientists and practitioners about state-of-the-art approaches and evaluation metrics to resolve disagreements between explanations.

